<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Vithursan -- Personal Site  &#8211; ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning </title>
<meta name="description" content="Gradient optimized Hebbian plasticity to help mitigate catastrophic forgetting.">
<meta name="keywords" content="">
<!-- Twitter Cards -->
	
		<meta name="twitter:card" content="summary">
		<meta name="twitter:image" content=
			
				
						"http://localhost:4000/images/"
				
			
		>
	
	<meta name="twitter:title" content="ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning">
	<meta name="twitter:description" content="Gradient optimized Hebbian plasticity to help mitigate catastrophic forgetting.">
	<meta name="twitter:creator" content="@vithursant19">


<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning">
<meta property="og:description" content="Gradient optimized Hebbian plasticity to help mitigate catastrophic forgetting.">
<meta property="og:url" content="http://localhost:4000/dhp-softmax/">
<meta property="og:site_name" content="Vithursan -- Personal Site">





<link rel="canonical" href="http://localhost:4000/dhp-softmax/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Vithursan -- Personal Site Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Google Webfonts -->
<link href='https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>
<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/academicons.css" />

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://localhost:4000/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://localhost:4000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">

<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">

<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="192x192" href="http://localhost:4000/images/apple-icon-precomposed.png">

</head>

<body class="post">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://localhost:4000">Vithursan -- Personal Site</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				<li><a href="http://localhost:4000/cv/" >CV</a></li>
		        
				<li><a href="http://localhost:4000/publications/" >Publications</a></li>
		        
				<li><a href="http://localhost:4000/code/" >Code</a></li>
		        
				<li><a href="http://localhost:4000/blog/" >Blog</a></li>
		        
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    
	<img src="http://localhost:4000/images/jordi.png" class="bio-photo" alt="Vithursan Thangarasa bio photo"></a>

<h3>Vithursan Thangarasa</h3>
<p>Originally from Toronto, Canada, and currently based in the San Francisco Bay Area, I am deeply passionate about neural network compression, large-scale foundation models, and enhancing the efficiency of training large neural networks, with a keen interest in generative AI.</p>
<a href="http://twitter.com/vithursant19" class="author-social" target="_blank"><i class="fa fa-twitter-square"></i> Twitter</a>
<a href="http://scholar.google.com/citations?user=UUKxm4gAAAAJ&hl=en" class="author-social" target="_blank"><i class="ai ai-google-scholar-square"></i>&nbsp; Google Scholar</a>

<a href="http://linkedin.com/in/vithursant" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>


<a href="http://github.com/vithursant" class="author-social" target="_blank"><i class="fa fa-github-square"></i> Github</a>






<a href="mailto:vithursan.thangarasa@gmail.com" class="author-social" target="_blank"><i class="fa fa-envelope-square"></i> E-Mail</a>


  </div>
  <article>
    <div class="headline-wrap">
      
        <h1><a href="http://localhost:4000/dhp-softmax/" rel="bookmark" title="ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning">ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning</a></h1>
      
    </div><!--/ .headline-wrap -->
    <div class="article-wrap">
      <p><i>Note: To learn more about continual learning, check out my blog post: </i> <a href="https://vithursant.com/continual-learning-overview/" target="_blank">Continual Lifelong Learning with Deep Neural Nets</a>.</p>

<p><br />
Deep neural networks (DNNs) are prone to “forgetting” past knowledge, which has
been coined in the literature as <i>catastrophic forgetting</i> or
<i>catastrophic interference</i> [2, 4]. This limits the DNN’s ability to
perform continual lifelong learning as they face the “stability-plasticity”
dilemma when retaining memories. Stability refers to the preserving of existing
knowledge while, plasticity refers to integrating new knowledge.</p>

<p><br /> <b>Synaptic consolidation</b> and <b>Complementary Learning Systems
(CLS)</b> are two theories that have been proposed in the neuroscience
literature to explain how humans perform continual learning. The first theory
proposes that a proportion of synapses in our neocortex becomes less
<i>plastic</i> to retain information for a longer timescale. The second theory
suggests that two learning systems are present in our brain: 1) the neocortex
slowly learns generalizable structured knowledge 2) the hippocampus performs
rapid learning of new experiences. The experiences stored in the the hippocampus
are consolidated and replayed to the neocortex in the form of episodic memories
to reinforce the synaptic connections.</p>

<p><br />
One of the fundamental premises of neuroscience is <b>Hebbian learning</b> [3], which
suggests that learning and memory in biological neural networks are
attributed to weight plasticity, that is, the modification of the strength of
existing synapses according to variants of Hebb’s rule [7, 8].</p>

<p><br /> <b>Quick Summary</b> <br /> <br /> In our paper, we extend
<i>differentiable plasticity</i> [6] to a continual learning setting and develop
a model that is able to adapt quickly to changing environments as well as
consolidating past knowledge by dynamically adjusting the plasticity of
synapses. In the softmax layer, we augment the traditional (slow) weights used
to train DNNs with a set of plastic (fast) weights using <b>Differentiable
Hebbian Plasticity (DHP)</b>. This allows the plastic weights to behave as an
auto-associative memory that can rapidly bind deep representations of the data
from the penultimate layer to the class labels. We call this new softmax layer
as the <b>DHP-Softmax</b>. We also show the flexibility of our model by
combining it with popular task-specific synaptic consolidation methods in the
literature such as: elastic weight consolidation (EWC) [4, 9], synaptic
intelligence (SI) [10] and memory aware synapses (MAS) [1]. Our model is
implemented using the PyTorch framework and trained on a single Nvidia Titan V.</p>

<p><br /> <b>Our Model</b> <br /></p>

<p><br /> Each synaptic connection in our model is composed of two weights:</p>

<ul>
  <li>The slow weights \(\theta \in \mathbb{R}^{m \times d}\), where \(m\) is the
number of units in the final hidden layer and \(d\) is the number of classes.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize fixed (slow changing) weights with He initialization.
</span><span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_features</span><span class="p">))</span>
<span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">kaiming_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">w</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</code></pre></div></div>
<ul>
  <li>The Hebbian plastic component of the same cardinality as the slow weights and
made up of two components:
    <ul>
      <li>Hebbian trace, \(\mathrm{Hebb}\) – accumulates the mean hidden activations
of the mean activations of the penultimate layer for each target label in
the mini-batch during training which are denoted by \(\tilde{h} \in \mathbb{R}^{1
\times m}\) (using Algorithm 1 as shown below).</li>
      <li>Plasticity coefficient, \(\alpha\) – adjusts the magnitude of
\(\mathrm{Hebb}\).</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The alpha scaling coefficients of the plastic connections.
</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">((</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_features</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># The learning rate of plasticity (the same for all plastic connections)
</span><span class="n">self</span><span class="p">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">eta_rate</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><br />
Given the activation of each neuron in $h$ at the pre-synaptic connection $i$,
the unnormalized log probabilities $z$ at the post-synaptic connection $j$ can
be computed as follows:</p>

\[z_j = \sum_{i = 1}^{m}(\underbrace{\theta_{i,j}}_{\text{slow}} + \underbrace{\alpha_{i,j}\mathrm{Hebb}_{i,j}}_{\text{plastic (fast)}})h_i\]

<p><br />
Then, we apply the softmax function on \(z\) to obtain the desired logits
\(\hat{y}\) thus, \(\hat{y} = \mathrm{softmax}(z)\). The parameters
\(\alpha_{i,j}\) and \(\theta_{i,j}\) are optimized by gradient descent as the
model is trained sequentially on different tasks \(T_{1:nmax}\), where \(nmax\)
is the maximum number of tasks in the respective continual learning benchmarks
we experimented on.</p>

<p><br /> <b>Hebbian Update Rule</b> <br /></p>

<p><br />
The Hebbian traces \(\mathrm{Hebb}\) are updated as follows:</p>

\[\mathrm{Hebb}_{i,j} := (1-\eta)\mathrm{Hebb}_{i,j} +  \eta \tilde{h}_{i,j}\]

<p><br />
where, \(\eta\) is the Hebbian learning rate that learns how quickly to acquire new
experiences into the plastic component. Here, \(\eta\) also behaves as a decay term to
prevent a positive feedback loop in the \(\mathrm{Hebb}\).</p>

<p><br />
At the start of training the first task $T_{1}$, we initialize \(\mathrm{Hebb}\) to zero:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">initial_zero_hebb</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="k">return</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_features</span><span class="p">),</span> 
                <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<p>Then we update the Hebbian traces in the forward pass using Algorithm 1 (see
below). On line 6, we perform the Hebbian update for the corresponding class,
\(c\). This hebbian update method forms a compressed episodic memory in the
\(\mathrm{Hebb}\) that represents the memory traces for each unique class,
\(c\), in the mini-batch.  Across the model’s lifetime, we only update
\(\mathrm{Hebb}\) during training and during test time, we use the most recent
Hebbian traces to perform inference.</p>

<p><br />
\(\hspace{60pt}\) <img src="/images/alg1.png" alt="Frame" height="400px" width="400px" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">hebb</span><span class="p">):</span>
    <span class="c1"># Only update Hebbian traces during training.
</span>    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
            <span class="c1"># Get indices of corresponding class, c, in y.
</span>            <span class="n">y_c_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">).</span><span class="nf">nonzero</span><span class="p">()</span>
            <span class="c1"># Count total occurences of corresponding class, c in y.
</span>            <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">h_bar</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">div</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">y_c_idx</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">s</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
                <span class="n">hebb</span><span class="p">[:,</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">mul</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">eta</span><span class="p">),</span> 
                                <span class="n">hebb</span><span class="p">[:,</span><span class="n">c</span><span class="p">].</span><span class="nf">clone</span><span class="p">()),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mul</span><span class="p">(</span><span class="n">h_bar</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">eta</span><span class="p">))</span> 

    <span class="c1"># Compute softmax pre-activations with plastic (fast) weights.
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mul</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">hebb</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">hebb</span>
</code></pre></div></div>

<p><br /> <b>Hebbian Update Visualization</b> <br />
<br />
An example of a Hebbian update for the active class c = 1 (see Line 4 in Algorithm 1):
\(\hspace{60pt}\) <img src="/images/icmldraw19_1.svg" alt="Frame" height="800px" width="800px" /></p>

<p><br /> <b>Updated Quadratic Loss</b> <br /></p>

<p><br />
Following the quadratic loss in the existing work for overcoming catastrophic forgetting such as EWC,
Online EWC, SI and MAS, we regularize the loss, \(\mathcal{L}^{n}(\theta, \alpha, \eta)\), where
\(\Omega_{i,j}\) is an importance measure for each slow weight \(\theta_{i,j}\)
and determines how plastic the connections should be. Here, least plastic
weights can retain memories for a longer period of time whereas, more plastic
weights are considered less important.</p>

\[\tilde{\mathcal{L}}^{n}(\theta, \alpha, \eta) = \mathcal{L}^{n}(\theta, \alpha, \eta) + \underbrace{\lambda\sum_{i,j}\Omega_{i,j}(\theta_{i,j}^{n} - \theta_{i,j}^{n-1})^{2}}_{\text{regularizer}}\]

<p><br />
where, \(\theta_{i,j}^{n-1}\) are the learned network parameters after training on the previous \(n − 1\) tasks and \(\lambda\) is a hyperparameter for the regularizer to control the amount of forgetting.</p>

<p><br />
We adapt these existing synaptic consolidation approaches to DHP-Softmax and
only compute the \(\Omega_{i,j}\) on the slow weights of the network. The
plastic part of our model can alleviate catastrophic forgetting of learned
classes by optimizing the plasticity of the synaptic connections.</p>

<p><br /> <b>Experiments: SplitMNIST Benchmark</b> <br />
<br />
A sequence of \(T_{n=1:5}\) tasks are generated by splitting the original MNIST
training dataset into a sequence of 5 binary classification tasks: \(T_1 =
\{0/1\}\), \(T_2 = \{2/3\}\), \(T_3 = \{4/5\}\), \(T_4 = \{6/7\}\) and \(T_5 =
\{8/9\}\), making the output spaces disjoint between tasks. We trained a
multi-headed MLP network with two hidden layers of 256 ReLU nonlinearities each,
and a cross-entropy loss. We compute the cross-entropy loss,
\(\mathcal{L}(\theta)\), at the softmax output layer for the digits present in
the current task, \(T_n\). We train the network sequentially on all 5 tasks
\(T_{n=1:5}\) with mini-batches of size 64 and optimized using plain SGD with a
fixed learning rate of 0.01 for 10 epochs. More details on experimental setup
and hyperparameter settings can be found in the Appendix section our paper.</p>

<p><br />
\(\hspace{60pt}\) <img src="/images/benchmark.svg" alt="Frame" height="400px" width="400px" /></p>

<p><br />
We observe that DHP Softmax provides a 4.7% improvement on test performance
compared to a finetuned MLP network (refer to the graph above). Also, combining
DHP Softmax with task-specific consolidation consistently improves performance
across all tasks \(T_{n=1:5}\).</p>

<p><br /><b>Conclusion</b><br />
We show that catastrophic forgetting can be alleiviated by adding our DHP
Softmax to a DNN. Also, we demonstrate the flexibility and simplicity of our
approach by combining it with Online EWC, SI and MAS to regularize the slow
weights of the network. We do not introduce any additional hyperparameters as
all of the hyperparameters of the plastic component are learned dynamically by
the network during training. Finally, we hope this will foster new progress in
continual learning where, methods involving gradient-optimized Hebbian
plasticity can be used for learning and memory in DNNs.</p>

<p><br /><b>Future Work</b><br />
A natural extension of our work would be to apply DHP throughout all of the
layers in a feed forward neural network. Also, the current model relies on
labels to “auto-associate” the classes to deep representations. An interesting
line of work would be to perform the Hebbian update with a self-supervised
representation learning technique. It would also be interesting to see how this
approach will perform in the truly online setting where the network is expected
to maximize forward transfer from a small number of data points to learn new
tasks efficiently in a continual learning environment.</p>

<p><br /><b>More Details</b><br />
You can also read our <a href="" target="_blank">ICML 2019 Adaptive and
Multi-Task Learning: Algorithms &amp; Systems Workshop</a> paper <a href="https://openreview.net/forum?id=r1x-E5Ss34" target="_blank"><b>Differentiable
Hebbian Plasticity for Continual Learning</b></a>.</p>

<p><br /><b>References</b><br /></p>

<p><br />
[1] Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and
Tuytelaars, T. Memory aware synapses: Learning what
(not) to forget. In <i>The European Conference on Computer
Vision (ECCV)</i>, September 2018.</p>

<p><br />
[2] French, R. Catastrophic forgetting in connectionist networks. <i>Trends in Cognitive Sciences</i>, 3(4):128–135, 1999.</p>

<p><br />
[3] Hebb, D. O. <i>The organization of behavior; a neuropsychological theory</i>. Wiley, Oxford, England, 1949.</p>

<p><br />
[4] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho,
T., Grabska-Barwinska, A., Hassabis, D., Clopath, C.,
Kumaran, D., and Hadsell, R. Overcoming catastrophic
forgetting in neural networks. <i>Proceedings of the National Academy of Sciences (PNAS)</i>, 114(13):3521–3526,
March 2017.</p>

<p><br />
[5] McCloskey, M. and Cohen, N. J. Catastrophic interference in connectionist networks: The sequential learning problem. <i>The Psychology of Learning and Motivation</i>, 24: 104–169, 1989.</p>

<p><br />
[6] Miconi, T., Stanley, K. O., and Clune, J. Differentiable plasticity: training plastic neural networks with backpropagation. In <i>Proceedings of the 35th International Conference
on Machine Learning (ICML)</i>, pp. 3556–3565, 2018.</p>

<p><br />
[7] Oja, E. Oja learning rule. Scholarpedia, 3(3):3612, 2008</p>

<p><br />
[8] Paulsen, O. and Sejnowski, T. J. Natural patterns of activity
and long-term synaptic plasticity. <i>Current Opinion in
Neurobiology</i>, 10(2):172 – 180, 2000.</p>

<p><br />
[9] Schwarz, J., Czarnecki, W., Luketina, J., GrabskaBarwinska, A., Teh, Y. W., Pascanu, R., and Hadsell, R.
Progress &amp; compress: A scalable framework for continual
learning. In <i>Proceedings of the 35th International Conference on Machine Learning (ICML)</i>, pp. 4535–4544,
2018.</p>

<p><br />
[10] Zenke, F., Poole, B., and Ganguli, S. Continual learning
through synaptic intelligence. In <i>Proceedings of the 34th
International Conference on Machine Learning (ICML)</i>,
pp. 3987–3995, 2017.</p>

      <hr />
      <footer role="contentinfo">
        <div class="article-author-bottom">
          
	<img src="http://localhost:4000/images/jordi.png" class="bio-photo" alt="Vithursan Thangarasa bio photo"></a>

<h3>Vithursan Thangarasa</h3>
<p>Originally from Toronto, Canada, and currently based in the San Francisco Bay Area, I am deeply passionate about neural network compression, large-scale foundation models, and enhancing the efficiency of training large neural networks, with a keen interest in generative AI.</p>
<a href="http://twitter.com/vithursant19" class="author-social" target="_blank"><i class="fa fa-twitter-square"></i> Twitter</a>
<a href="http://scholar.google.com/citations?user=UUKxm4gAAAAJ&hl=en" class="author-social" target="_blank"><i class="ai ai-google-scholar-square"></i>&nbsp; Google Scholar</a>

<a href="http://linkedin.com/in/vithursant" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>


<a href="http://github.com/vithursant" class="author-social" target="_blank"><i class="fa fa-github-square"></i> Github</a>






<a href="mailto:vithursan.thangarasa@gmail.com" class="author-social" target="_blank"><i class="fa fa-envelope-square"></i> E-Mail</a>


        </div>
        <p class="byline"><strong>ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning</strong> was published on <time datetime="2019-05-28T00:00:00-07:00">May 28, 2019</time> by <a href="http://localhost:4000" title="About Vithursan Thangarasa">Vithursan Thangarasa</a>.</p>
      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  <footer>
    <span>&copy; 2024 Vithursan Thangarasa. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl = 
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-104881273-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<script type="text/javascript"
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  processEscapes: true},
  jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
  extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
  TeX: {
  extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
  equationNumbers: {
  autoNumber: "AMS"
  }
  }
  });
</script>


  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'vithursant'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

	        

</body>
</html>