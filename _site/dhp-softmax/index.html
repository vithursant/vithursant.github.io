<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Vithursan -- Personal Site  &#8211; ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning </title>
<meta name="description" content="Gradient optimized Hebbian plasticity to help mitigate catastrophic forgetting.">
<meta name="keywords" content="">
<!-- Twitter Cards -->
	
		<meta name="twitter:card" content="summary">
		<meta name="twitter:image" content=
			
				
						"http://localhost:5000/images/"
				
			
		>
	
	<meta name="twitter:title" content="ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning">
	<meta name="twitter:description" content="Gradient optimized Hebbian plasticity to help mitigate catastrophic forgetting.">
	<meta name="twitter:creator" content="@vithursant19">


<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning">
<meta property="og:description" content="Gradient optimized Hebbian plasticity to help mitigate catastrophic forgetting.">
<meta property="og:url" content="http://localhost:5000/dhp-softmax/">
<meta property="og:site_name" content="Vithursan -- Personal Site">





<link rel="canonical" href="http://localhost:5000/dhp-softmax/">
<link href="http://localhost:5000/feed.xml" type="application/atom+xml" rel="alternate" title="Vithursan -- Personal Site Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Google Webfonts -->
<link href='https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>
<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:5000/assets/css/main.min.css">
<link rel="stylesheet" href="http://localhost:5000/assets/academicons.css" />

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://localhost:5000/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://localhost:5000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:5000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">

<link rel="shortcut icon" href="http://localhost:5000/favicon.ico">
<link rel="shortcut icon" href="http://localhost:5000/favicon.png">

<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="192x192" href="http://localhost:5000/images/apple-icon-precomposed.png">

</head>

<body class="post">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://localhost:5000">Vithursan -- Personal Site</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				<li><a href="http://localhost:5000/cv/" >CV</a></li>
		        
				<li><a href="http://localhost:5000/publications/" >Publications</a></li>
		        
				<li><a href="http://localhost:5000/code/" >Code</a></li>
		        
				<li><a href="http://localhost:5000/blog/" >Blog</a></li>
		        
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    
	<img src="http://localhost:5000/images/jordi.png" class="bio-photo" alt="Vithursan Thangarasa bio photo"></a>

<h3>Vithursan Thangarasa</h3>
<p>I'm from Toronto, Canada. Currently in the San Francisco Bay Area, California. Passionate about machine learning, computer vision, technology and basketball.</p>
<a href="http://twitter.com/vithursant19" class="author-social" target="_blank"><i class="fa fa-twitter-square"></i> Twitter</a>
<a href="http://scholar.google.es/citations?user=UUKxm4gAAAAJ" class="author-social" target="_blank"><i class="ai ai-google-scholar-square"></i>&nbsp; Google Scholar</a>

<a href="http://linkedin.com/in/vithursant" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>


<a href="http://github.com/vithursant" class="author-social" target="_blank"><i class="fa fa-github-square"></i> Github</a>






<a href="mailto:vithursan.thangarasa@gmail.com" class="author-social" target="_blank"><i class="fa fa-envelope-square"></i> E-Mail</a>


  </div>
  <article>
    <div class="headline-wrap">
      
        <h1><a href="http://localhost:5000/dhp-softmax/" rel="bookmark" title="ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning">ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning</a></h1>
      
    </div><!--/ .headline-wrap -->
    <div class="article-wrap">
      <p>Deep neural networks (DNNs) are known to be prone to “forgetting”, which has been
coined in the literature as <i>catastrophic forgetting</i> or <i>catastrophic interference</i> [2, 4].
This limits the DNN’s ability to perform continual lifelong learning as they face
the “stability-plasticity” dilemma when retaining memories. Stability refers to the preserving of existing knowledge while, plasticity refers to integrating new knowledge.</p>

<p><br />
<b>Synaptic consolidation</b> and <b>Complementary Learning Systems (CLS)</b> are two theories that have been proposed in the neuroscience literature to explain how  humans perform continual learning. The first theory proposes that a proportion of synapses in our neocortex becomes less <i>plastic</i> to retain information for a longer timescale. The second theory suggests that two learning systems are present in our brain: 1) the neocortex slowly learns generalizable structured knowledge 2) the hippocampus performs rapid learning of new experiences. The experiences stored in the the hippocampus are consolidated and replayed to the neocortex in the form of episodic memories to reinforce the synaptic connections.</p>

<p><br /> <b>Quick Summary</b> <br />
<br />
In our paper, we extend <i>differentiable plasticity</i> [4] to a continual learning setting and develop a model that is able to adapt quickly to changing environments as well as consolidating past knowledge by dynapically adjusting the plasticity of synapses. In the softmax layer, we augment the traditional (slow) weights used to train DNNs with a set of plastic (fast) weights using <b>Differentiable Hebbian Plasticity (DHP)</b>. This allows the plastic weights to behave as an auto-associative memory that can rapidly bind deep representations of the data from the penultimate layer to the class labels. We call this new softmax layer as the <b>DHP-Softmax</b>. We also show the flexibility of our model by combining it with popular task-specific synaptic consolidation methods in the literature such as: elastic weight consolidation (EWC) [3, 6], synaptic intelligence (SI) [7] and memory aware synapses (MAS) [1]. Our model is implemented using the PyTorch framework and trained on 
a single Nvidia Titan V.</p>

<p><br /> <b>Our Model</b> <br /></p>

<p><br />
Each synaptic connection in our model is composed of two weights:</p>

<ul>
  <li>The slow weights <script type="math/tex">\theta \in \mathbb{R}^{m \times d}</script>, where <script type="math/tex">m</script> is the number of units in the final hidden layer and <script type="math/tex">d</script> is the number of classes.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize fixed (slow changing) weights with He initialization.
</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">))</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</code></pre></div></div>
<ul>
  <li>The Hebbian plastic component of the same cardinality as the
slow weights and made up of two components:
    <ul>
      <li>Hebbian trace, <script type="math/tex">\mathrm{Hebb}</script> – accumulates the mean hidden activations of the mean activations of the penultimate layer for each target label in the mini-batch during training which are denoted by <script type="math/tex">h \in \mathbb{R}^{1 \times m}</script> (using Algorithm 1 as shown below).</li>
      <li>Plasticity coefficient, <script type="math/tex">\alpha</script> – adjusts the magnitude of <script type="math/tex">\mathrm{Hebb}</script>.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The alpha scaling coefficients of the plastic connections.
</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">((</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># The learning rate of plasticity (the same for all plastic connections)
</span><span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">eta_rate</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><br />
Given the activation of each neuron in $h$ at the pre-synaptic connection $i$, the
unnormalized log probabilities $z$ at the post-synaptic connection $j$ can be computed as follows:</p>

<script type="math/tex; mode=display">z_j = \sum_{i \in \text{inputs to} \ j}(\underbrace{\theta_{i,j}}_{\text{slow}} + \underbrace{\alpha_{i,j}\mathrm{Hebb}_{i,j}}_{\text{fast}})h_i</script>

<p><br />
Then, we apply the softmax function on <script type="math/tex">z</script> to obtain the desired logits <script type="math/tex">\hat{y}</script> thus, <script type="math/tex">\hat{y} = \mathrm{softmax}(z)</script>. The parameters <script type="math/tex">\alpha_{i,j}</script> and <script type="math/tex">\theta_{i,j}</script> are optimized by gradient descent as the model is trained sequentially on different tasks <script type="math/tex">T_{1:nmax}</script>, where <script type="math/tex">nmax</script> is the maximum number of tasks in the respective continual learning benchmarks we experimented on.</p>

<p><br /> <b>Hebbian Update Rule</b> <br /></p>

<p><br />
At the start of training the first task $T_{1}$, we initialize <script type="math/tex">\mathrm{Hebb}</script> to zero:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">initial_zero_hebb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">'cuda'</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="k">return</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">),</span> 
                <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<p>Then we update the Hebbian traces in the forward pass using Algorithm 1 (see below). On line 6, we perform the Hebbian update for the corresponding class, <script type="math/tex">c</script>. This hebbian update method forms a compressed episodic memory in the <script type="math/tex">\mathrm{Hebb}</script> that represents the memory traces for each unique class, <script type="math/tex">c</script>, in the mini-batch.  Across the model’s lifetime, we only update <script type="math/tex">\mathrm{Hebb}</script> during training and during test time, we use the most recent Hebbian traces to perform inference.</p>

<p><br />
<script type="math/tex">\hspace{60pt}</script> <img src="/images/alg1.png" alt="Frame" height="400px" width="400px" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">hebb</span><span class="p">):</span>
    <span class="c1"># Only update Hebbian traces during training.
</span>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
            <span class="c1"># Get indices of corresponding class, c, in y.
</span>            <span class="n">y_c_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
            <span class="c1"># Count total occurences of corresponding class, c in y.
</span>            <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">h_bar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">y_c_idx</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">s</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                <span class="n">hebb</span><span class="p">[:,</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span><span class="p">),</span> 
                                <span class="n">hebb</span><span class="p">[:,</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()),</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">h_bar</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span><span class="p">))</span> 

    <span class="c1"># Compute softmax pre-activations with plastic (fast) weights.
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">hebb</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">hebb</span>
</code></pre></div></div>

<p><br /> <b>Updated Loss</b> <br /></p>

<p><br />
Following the existing work for overcoming
catastrophic forgetting such as EWC, Online EWC, SI and
MAS, we regularize the loss, <script type="math/tex">\mathcal{L}(\theta)</script>, where <script type="math/tex">\Omega_{i,j}</script> is an importance measure for each slow weight <script type="math/tex">\theta_{i,j}</script> and determines how plastic the connections should be. Here, least plastic weights can retain memories for a longer period of time whereas, more plastic weights are considered less important.</p>

<script type="math/tex; mode=display">\mathcal{L}(\theta) = \mathcal{L}^{n}(\theta) + \underbrace{\lambda\sum_{i,j}\Omega_{i,j}(\theta_{i,j}^{n} - \theta_{i,j}^{n-1})^{2}}_{\text{regularizer}}</script>

<p><br />
where, <script type="math/tex">\theta_{i,j}^{n-1}</script> are the learned network parameters after training on the previous <script type="math/tex">n − 1</script> tasks and <script type="math/tex">\lambda</script> is a hyperparameter for the regularizer to control the amount of forgetting.</p>

<p><br />
We adapt these existing synaptic consolidation approaches to DHP-Softmax and only compute the <script type="math/tex">\Omega_{i,j}</script>
on the slow weights of the network. The plastic part of our
model can alleviate catastrophic forgetting of learned classes
by optimizing the plasticity of the synaptic connections.</p>

<p><br /> <b>Experiments: SplitMNIST Benchmark</b> <br />
<br />
A sequence of <script type="math/tex">T_{n=1:5}</script> tasks are generated
by splitting the original MNIST training dataset into a sequence of 5 binary
classification tasks: <script type="math/tex">T_1 = \{0/1\}</script>, <script type="math/tex">T_2 =
\{2/3\}</script>, <script type="math/tex">T_3 = \{4/5\}</script>, <script type="math/tex">T_4 = \{6/7\}</script> and <script type="math/tex">T_5 = \{8/9\}</script>, making
the output spaces disjoint between tasks. We trained a multi-headed MLP network 
with two hidden layers of 256 ReLU nonlinearities each, and a cross-entropy loss. 
We compute the cross-entropy loss, <script type="math/tex">\mathcal{L}(\theta)</script>, at the softmax output layer for the digits present in the current task, <script type="math/tex">T_n</script>. We train the network sequentially on all 5 tasks <script type="math/tex">T_{n=1:5}</script> with mini-batches of size 64 and optimized using plain SGD with a fixed learning rate of 0.01 for 10 epochs. More details on experimental setup and hyperparameter settings can be found in the Appendix section our paper.</p>

<p><br />
<script type="math/tex">\hspace{60pt}</script> <img src="/images/benchmark.svg" alt="Frame" height="400px" width="400px" /></p>

<p><br />
We observe that DHP Softmax provides a 4.7% improvement on test performance compared
to a finetuned MLP network (refer to the graph above). Also, combining
DHP Softmax with task-specific consolidation consistently improves performance across all tasks <script type="math/tex">T_{n=1:5}</script>.</p>

<p><br /><b>References</b><br /></p>

<p><br />
[1] Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and
Tuytelaars, T. Memory aware synapses: Learning what
(not) to forget. In <i>The European Conference on Computer
Vision (ECCV)</i>, September 2018.</p>

<p><br />
[2] French, R. Catastrophic forgetting in connectionist networks. <i>Trends in Cognitive Sciences</i>, 3(4):128–135, 1999.</p>

<p><br />
[3] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho,
T., Grabska-Barwinska, A., Hassabis, D., Clopath, C.,
Kumaran, D., and Hadsell, R. Overcoming catastrophic
forgetting in neural networks. <i>Proceedings of the National Academy of Sciences (PNAS)</i>, 114(13):3521–3526,
March 2017.</p>

<p><br />
[4] McCloskey, M. and Cohen, N. J. Catastrophic interference in connectionist networks: The sequential learning problem. <i>The Psychology of Learning and Motivation</i>, 24: 104–169, 1989.</p>

<p>[5] Miconi, T., Stanley, K. O., and Clune, J. Differentiable plasticity: training plastic neural networks with backpropagation. In <i>Proceedings of the 35th International Conference
on Machine Learning (ICML)</i>, pp. 3556–3565, 2018.</p>

<p><br />
[6] Schwarz, J., Czarnecki, W., Luketina, J., GrabskaBarwinska, A., Teh, Y. W., Pascanu, R., and Hadsell, R.
Progress &amp; compress: A scalable framework for continual
learning. In <i>Proceedings of the 35th International Conference on Machine Learning (ICML)</i>, pp. 4535–4544,
2018.</p>

<p><br />
[7] Zenke, F., Poole, B., and Ganguli, S. Continual learning
through synaptic intelligence. In <i>Proceedings of the 34th
International Conference on Machine Learning (ICML)</i>,
pp. 3987–3995, 2017.</p>

      <hr />
      <footer role="contentinfo">
        <div class="article-author-bottom">
          
	<img src="http://localhost:5000/images/jordi.png" class="bio-photo" alt="Vithursan Thangarasa bio photo"></a>

<h3>Vithursan Thangarasa</h3>
<p>I'm from Toronto, Canada. Currently in the San Francisco Bay Area, California. Passionate about machine learning, computer vision, technology and basketball.</p>
<a href="http://twitter.com/vithursant19" class="author-social" target="_blank"><i class="fa fa-twitter-square"></i> Twitter</a>
<a href="http://scholar.google.es/citations?user=UUKxm4gAAAAJ" class="author-social" target="_blank"><i class="ai ai-google-scholar-square"></i>&nbsp; Google Scholar</a>

<a href="http://linkedin.com/in/vithursant" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>


<a href="http://github.com/vithursant" class="author-social" target="_blank"><i class="fa fa-github-square"></i> Github</a>






<a href="mailto:vithursan.thangarasa@gmail.com" class="author-social" target="_blank"><i class="fa fa-envelope-square"></i> E-Mail</a>


        </div>
        <p class="byline"><strong>ICML AMTL 2019 - Differentiable Hebbian Plasticity for Continual Learning</strong> was published on <time datetime="2019-05-28T00:00:00+02:00">May 28, 2019</time> by <a href="http://localhost:5000" title="About Vithursan Thangarasa">Vithursan Thangarasa</a>.</p>
      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  <footer>
    <span>&copy; 2019 Vithursan Thangarasa. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:5000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:5000/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl = 
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-104881273-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<script type="text/javascript"
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  processEscapes: true},
  jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
  extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
  TeX: {
  extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
  equationNumbers: {
  autoNumber: "AMS"
  }
  }
  });
</script>


  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'vithursant'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

	        

</body>
</html>